GPU detected
Loading data to memory
  0%|                                                                       | 0/196 [00:00<?, ?it/s]WARNING:tensorflow:Gradients do not exist for variables ['model/dense_3/kernel:0', 'model/dense_3/bias:0', 'model/dense_6/kernel:0', 'model/dense_6/bias:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['model/dense_3/kernel:0', 'model/dense_3/bias:0', 'model/dense_6/kernel:0', 'model/dense_6/bias:0'] when minimizing the loss.
  1%|â–Ž                                                              | 1/196 [00:06<20:49,  6.41s/it]Epoch 1 / 100; Avg_loss: 2.755154:   1%|â–                           | 1/196 [00:06<20:49,  6.41s/it]Epoch 1 / 100; Avg_loss: 2.755154:   1%|â–Ž                           | 2/196 [00:07<15:49,  4.90s/it]Epoch 1 / 100; Avg_loss: 2.700721:   1%|â–Ž                           | 2/196 [00:07<15:49,  4.90s/it]Epoch 1 / 100; Avg_loss: 2.700721:   2%|â–                           | 3/196 [00:09<12:18,  3.83s/it]Epoch 1 / 100; Avg_loss: 2.585327:   2%|â–                           | 3/196 [00:09<12:18,  3.83s/it]Epoch 1 / 100; Avg_loss: 2.585327:   2%|â–Œ                           | 4/196 [00:10<09:52,  3.09s/it]Epoch 1 / 100; Avg_loss: 2.474843:   2%|â–Œ                           | 4/196 [00:10<09:52,  3.09s/it]Epoch 1 / 100; Avg_loss: 2.474843:   3%|â–‹                           | 5/196 [00:11<08:11,  2.57s/it]Epoch 1 / 100; Avg_loss: 2.401468:   3%|â–‹                           | 5/196 [00:11<08:11,  2.57s/it]Epoch 1 / 100; Avg_loss: 2.401468:   3%|â–Š                           | 6/196 [00:13<07:00,  2.21s/it]Epoch 1 / 100; Avg_loss: 2.342385:   3%|â–Š                           | 6/196 [00:13<07:00,  2.21s/it]Epoch 1 / 100; Avg_loss: 2.342385:   4%|â–ˆ                           | 7/196 [00:14<06:09,  1.96s/it]Epoch 1 / 100; Avg_loss: 2.278843:   4%|â–ˆ                           | 7/196 [00:14<06:09,  1.96s/it]Epoch 1 / 100; Avg_loss: 2.278843:   4%|â–ˆâ–                          | 8/196 [00:15<05:32,  1.77s/it]Epoch 1 / 100; Avg_loss: 2.224633:   4%|â–ˆâ–                          | 8/196 [00:15<05:32,  1.77s/it]Epoch 1 / 100; Avg_loss: 2.224633:   5%|â–ˆâ–Ž                          | 9/196 [00:17<05:07,  1.65s/it]Epoch 1 / 100; Avg_loss: 2.187388:   5%|â–ˆâ–Ž                          | 9/196 [00:17<05:07,  1.65s/it]Epoch 1 / 100; Avg_loss: 2.187388:   5%|â–ˆâ–                         | 10/196 [00:18<04:48,  1.55s/it]Epoch 1 / 100; Avg_loss: 2.134112:   5%|â–ˆâ–                         | 10/196 [00:18<04:48,  1.55s/it]Epoch 1 / 100; Avg_loss: 2.134112:   6%|â–ˆâ–Œ                         | 11/196 [00:20<04:39,  1.51s/it]Epoch 1 / 100; Avg_loss: 2.092053:   6%|â–ˆâ–Œ                         | 11/196 [00:20<04:39,  1.51s/it]Epoch 1 / 100; Avg_loss: 2.092053:   6%|â–ˆâ–‹                         | 12/196 [00:21<04:31,  1.47s/it]Epoch 1 / 100; Avg_loss: 2.046513:   6%|â–ˆâ–‹                         | 12/196 [00:21<04:31,  1.47s/it]Epoch 1 / 100; Avg_loss: 2.046513:   7%|â–ˆâ–Š                         | 13/196 [00:22<04:22,  1.43s/it]Epoch 1 / 100; Avg_loss: 2.017805:   7%|â–ˆâ–Š                         | 13/196 [00:22<04:22,  1.43s/it]Epoch 1 / 100; Avg_loss: 2.017805:   7%|â–ˆâ–‰                         | 14/196 [00:24<04:15,  1.40s/it]Epoch 1 / 100; Avg_loss: 1.988877:   7%|â–ˆâ–‰                         | 14/196 [00:24<04:15,  1.40s/it]Epoch 1 / 100; Avg_loss: 1.988877:   8%|â–ˆâ–ˆ                         | 15/196 [00:25<04:10,  1.38s/it]Epoch 1 / 100; Avg_loss: 1.960723:   8%|â–ˆâ–ˆ                         | 15/196 [00:25<04:10,  1.38s/it]Epoch 1 / 100; Avg_loss: 1.960723:   8%|â–ˆâ–ˆâ–                        | 16/196 [00:26<04:07,  1.38s/it]Epoch 1 / 100; Avg_loss: 1.933535:   8%|â–ˆâ–ˆâ–                        | 16/196 [00:26<04:07,  1.38s/it]Epoch 1 / 100; Avg_loss: 1.933535:   9%|â–ˆâ–ˆâ–Ž                        | 17/196 [00:28<04:03,  1.36s/it]Epoch 1 / 100; Avg_loss: 1.899303:   9%|â–ˆâ–ˆâ–Ž                        | 17/196 [00:28<04:03,  1.36s/it]Epoch 1 / 100; Avg_loss: 1.899303:   9%|â–ˆâ–ˆâ–                        | 18/196 [00:29<04:02,  1.36s/it]Epoch 1 / 100; Avg_loss: 1.874554:   9%|â–ˆâ–ˆâ–                        | 18/196 [00:29<04:02,  1.36s/it]Epoch 1 / 100; Avg_loss: 1.874554:  10%|â–ˆâ–ˆâ–Œ                        | 19/196 [00:30<04:01,  1.37s/it]Epoch 1 / 100; Avg_loss: 1.845022:  10%|â–ˆâ–ˆâ–Œ                        | 19/196 [00:30<04:01,  1.37s/it]Epoch 1 / 100; Avg_loss: 1.845022:  10%|â–ˆâ–ˆâ–Š                        | 20/196 [00:32<04:00,  1.37s/it]Epoch 1 / 100; Avg_loss: 1.817701:  10%|â–ˆâ–ˆâ–Š                        | 20/196 [00:32<04:00,  1.37s/it]Epoch 1 / 100; Avg_loss: 1.817701:  11%|â–ˆâ–ˆâ–‰                        | 21/196 [00:33<03:58,  1.36s/it]Epoch 1 / 100; Avg_loss: 1.794313:  11%|â–ˆâ–ˆâ–‰                        | 21/196 [00:33<03:58,  1.36s/it]Epoch 1 / 100; Avg_loss: 1.794313:  11%|â–ˆâ–ˆâ–ˆ                        | 22/196 [00:34<03:58,  1.37s/it]Epoch 1 / 100; Avg_loss: 1.771171:  11%|â–ˆâ–ˆâ–ˆ                        | 22/196 [00:34<03:58,  1.37s/it]Epoch 1 / 100; Avg_loss: 1.771171:  12%|â–ˆâ–ˆâ–ˆâ–                       | 23/196 [00:36<03:55,  1.36s/it]Epoch 1 / 100; Avg_loss: 1.748298:  12%|â–ˆâ–ˆâ–ˆâ–                       | 23/196 [00:36<03:55,  1.36s/it]Epoch 1 / 100; Avg_loss: 1.748298:  12%|â–ˆâ–ˆâ–ˆâ–Ž                       | 24/196 [00:37<03:53,  1.36s/it]Epoch 1 / 100; Avg_loss: 1.730439:  12%|â–ˆâ–ˆâ–ˆâ–Ž                       | 24/196 [00:37<03:53,  1.36s/it]Epoch 1 / 100; Avg_loss: 1.730439:  13%|â–ˆâ–ˆâ–ˆâ–                       | 25/196 [00:39<03:53,  1.37s/it]Epoch 1 / 100; Avg_loss: 1.712172:  13%|â–ˆâ–ˆâ–ˆâ–                       | 25/196 [00:39<03:53,  1.37s/it]Epoch 1 / 100; Avg_loss: 1.712172:  13%|â–ˆâ–ˆâ–ˆâ–Œ                       | 26/196 [00:40<03:56,  1.39s/it]Epoch 1 / 100; Avg_loss: 1.688889:  13%|â–ˆâ–ˆâ–ˆâ–Œ                       | 26/196 [00:40<03:56,  1.39s/it]Epoch 1 / 100; Avg_loss: 1.688889:  14%|â–ˆâ–ˆâ–ˆâ–‹                       | 27/196 [00:41<04:01,  1.43s/it]Epoch 1 / 100; Avg_loss: 1.670374:  14%|â–ˆâ–ˆâ–ˆâ–‹                       | 27/196 [00:41<04:01,  1.43s/it]Epoch 1 / 100; Avg_loss: 1.670374:  14%|â–ˆâ–ˆâ–ˆâ–Š                       | 28/196 [00:43<04:05,  1.46s/it]Epoch 1 / 100; Avg_loss: 1.651202:  14%|â–ˆâ–ˆâ–ˆâ–Š                       | 28/196 [00:43<04:05,  1.46s/it]Epoch 1 / 100; Avg_loss: 1.651202:  15%|â–ˆâ–ˆâ–ˆâ–‰                       | 29/196 [00:45<04:13,  1.52s/it]Epoch 1 / 100; Avg_loss: 1.628531:  15%|â–ˆâ–ˆâ–ˆâ–‰                       | 29/196 [00:45<04:13,  1.52s/it]Epoch 1 / 100; Avg_loss: 1.628531:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–                      | 30/196 [00:46<04:22,  1.58s/it]Epoch 1 / 100; Avg_loss: 1.606926:  15%|â–ˆâ–ˆâ–ˆâ–ˆâ–                      | 30/196 [00:46<04:22,  1.58s/it]Epoch 1 / 100; Avg_loss: 1.606926:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 31/196 [00:48<04:19,  1.58s/it]Epoch 1 / 100; Avg_loss: 1.587208:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž                      | 31/196 [00:48<04:19,  1.58s/it]Epoch 1 / 100; Avg_loss: 1.587208:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–                      | 32/196 [00:49<04:15,  1.56s/it]Epoch 1 / 100; Avg_loss: 1.569352:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–                      | 32/196 [00:49<04:15,  1.56s/it]Epoch 1 / 100; Avg_loss: 1.569352:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 33/196 [00:51<04:10,  1.53s/it]Epoch 1 / 100; Avg_loss: 1.551910:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 33/196 [00:51<04:10,  1.53s/it]Epoch 1 / 100; Avg_loss: 1.551910:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 34/196 [00:52<04:00,  1.49s/it]Epoch 1 / 100; Avg_loss: 1.536145:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹                      | 34/196 [00:52<04:00,  1.49s/it]Epoch 1 / 100; Avg_loss: 1.536145:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 35/196 [00:54<03:55,  1.47s/it]Epoch 1 / 100; Avg_loss: 1.517910:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 35/196 [00:54<03:55,  1.47s/it]Epoch 1 / 100; Avg_loss: 1.517910:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 36/196 [00:55<03:51,  1.44s/it]Epoch 1 / 100; Avg_loss: 1.499461:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 36/196 [00:55<03:51,  1.44s/it]Epoch 1 / 100; Avg_loss: 1.499461:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 37/196 [00:57<03:48,  1.44s/it]Epoch 1 / 100; Avg_loss: 1.483529:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 37/196 [00:57<03:48,  1.44s/it]Epoch 1 / 100; Avg_loss: 1.483529:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 38/196 [00:58<03:46,  1.43s/it]Epoch 1 / 100; Avg_loss: 1.468302:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 38/196 [00:58<03:46,  1.43s/it]Epoch 1 / 100; Avg_loss: 1.468302:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 39/196 [00:59<03:43,  1.42s/it]Epoch 1 / 100; Avg_loss: 1.455151:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                     | 39/196 [00:59<03:43,  1.42s/it]Epoch 1 / 100; Avg_loss: 1.455151:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 40/196 [01:01<03:40,  1.41s/it]Epoch 1 / 100; Avg_loss: 1.445579:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 40/196 [01:01<03:40,  1.41s/it]Epoch 1 / 100; Avg_loss: 1.445579:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 41/196 [01:02<03:40,  1.42s/it]Epoch 1 / 100; Avg_loss: 1.432818:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 41/196 [01:02<03:40,  1.42s/it]Epoch 1 / 100; Avg_loss: 1.432818:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 42/196 [01:04<03:40,  1.43s/it]Epoch 1 / 100; Avg_loss: 1.419122:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                     | 42/196 [01:04<03:40,  1.43s/it]Epoch 1 / 100; Avg_loss: 1.419122:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 43/196 [01:05<03:37,  1.42s/it]Epoch 1 / 100; Avg_loss: 1.406866:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 43/196 [01:05<03:37,  1.42s/it]Epoch 1 / 100; Avg_loss: 1.406866:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 44/196 [01:07<03:42,  1.46s/it]Epoch 1 / 100; Avg_loss: 1.392372:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 44/196 [01:07<03:42,  1.46s/it]Epoch 1 / 100; Avg_loss: 1.392372:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 45/196 [01:08<03:45,  1.49s/it]Epoch 1 / 100; Avg_loss: 1.379381:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 45/196 [01:08<03:45,  1.49s/it]Epoch 1 / 100; Avg_loss: 1.379381:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 46/196 [01:10<03:54,  1.57s/it]Epoch 1 / 100; Avg_loss: 1.367937:  23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                    | 46/196 [01:10<03:54,  1.57s/it]Epoch 1 / 100; Avg_loss: 1.367937:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 47/196 [01:11<03:53,  1.57s/it]Epoch 1 / 100; Avg_loss: 1.356046:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 47/196 [01:12<03:53,  1.57s/it]Epoch 1 / 100; Avg_loss: 1.356046:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 48/196 [01:13<03:51,  1.56s/it]Epoch 1 / 100; Avg_loss: 1.343635:  24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 48/196 [01:13<03:51,  1.56s/it]Epoch 1 / 100; Avg_loss: 1.343635:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 49/196 [01:14<03:43,  1.52s/it]Epoch 1 / 100; Avg_loss: 1.331957:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 49/196 [01:14<03:43,  1.52s/it]Epoch 1 / 100; Avg_loss: 1.331957:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 50/196 [01:16<03:41,  1.52s/it]Epoch 1 / 100; Avg_loss: 1.321131:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 50/196 [01:16<03:41,  1.52s/it]Epoch 1 / 100; Avg_loss: 1.321131:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 51/196 [01:17<03:38,  1.51s/it]Epoch 1 / 100; Avg_loss: nan:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 51/196 [01:17<03:38,  1.51s/it][0;31m---------------------------------------------------------------------------[0m
[0;31mInvalidArgumentError[0m                      Traceback (most recent call last)
[0;32m~/Desktop/IceCube_GNN/train_likelihood.py[0m in [0;36m<module>[0;34m[0m
[1;32m    229[0m     [0minputs[0m[0;34m,[0m [0mtargets[0m  [0;34m=[0m [0mbatch[0m[0;34m[0m[0;34m[0m[0m
[1;32m    230[0m     [0minputs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[[0m[0;34m:[0m[0;34m,[0m [0;34m:[0m[0;36m3[0m[0;34m][0m [0;34m=[0m [0minputs[0m[0;34m[[0m[0;36m0[0m[0;34m][0m[0;34m[[0m[0;34m:[0m[0;34m,[0m [0;34m:[0m[0;36m3[0m[0;34m][0m [0;34m/[0m [0;36m1000[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 231[0;31m     [0mout[0m              [0;34m=[0m [0mtrain_step[0m[0;34m([0m[0minputs[0m[0;34m,[0m [0mtargets[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    232[0m     [0mloss[0m            [0;34m+=[0m [0mout[0m[0;34m[0m[0;34m[0m[0m
[1;32m    233[0m [0;34m[0m[0m

[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py[0m in [0;36m__call__[0;34m(self, *args, **kwds)[0m
[1;32m    826[0m     [0mtracing_count[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mexperimental_get_tracing_count[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[1;32m    827[0m     [0;32mwith[0m [0mtrace[0m[0;34m.[0m[0mTrace[0m[0;34m([0m[0mself[0m[0;34m.[0m[0m_name[0m[0;34m)[0m [0;32mas[0m [0mtm[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 828[0;31m       [0mresult[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_call[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    829[0m       [0mcompiler[0m [0;34m=[0m [0;34m"xla"[0m [0;32mif[0m [0mself[0m[0;34m.[0m[0m_experimental_compile[0m [0;32melse[0m [0;34m"nonXla"[0m[0;34m[0m[0;34m[0m[0m
[1;32m    830[0m       [0mnew_tracing_count[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mexperimental_get_tracing_count[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m

[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py[0m in [0;36m_call[0;34m(self, *args, **kwds)[0m
[1;32m    853[0m       [0;31m# In this case we have created variables on the first call, so we run the[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[1;32m    854[0m       [0;31m# defunned version which is guaranteed to never create variables.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 855[0;31m       [0;32mreturn[0m [0mself[0m[0;34m.[0m[0m_stateless_fn[0m[0;34m([0m[0;34m*[0m[0margs[0m[0;34m,[0m [0;34m**[0m[0mkwds[0m[0;34m)[0m  [0;31m# pylint: disable=not-callable[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m    856[0m     [0;32melif[0m [0mself[0m[0;34m.[0m[0m_stateful_fn[0m [0;32mis[0m [0;32mnot[0m [0;32mNone[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    857[0m       [0;31m# Release the lock early so that multiple threads can perform the call[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m

[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py[0m in [0;36m__call__[0;34m(self, *args, **kwargs)[0m
[1;32m   2940[0m       (graph_function,
[1;32m   2941[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)
[0;32m-> 2942[0;31m     return graph_function._call_flat(
[0m[1;32m   2943[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
[1;32m   2944[0m [0;34m[0m[0m

[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py[0m in [0;36m_call_flat[0;34m(self, args, captured_inputs, cancellation_manager)[0m
[1;32m   1916[0m         and executing_eagerly):
[1;32m   1917[0m       [0;31m# No tape is watching; skip to running the function.[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m
[0;32m-> 1918[0;31m       return self._build_call_outputs(self._inference_function.call(
[0m[1;32m   1919[0m           ctx, args, cancellation_manager=cancellation_manager))
[1;32m   1920[0m     forward_backward = self._select_forward_and_backward_functions(

[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py[0m in [0;36mcall[0;34m(self, ctx, args, cancellation_manager)[0m
[1;32m    553[0m       [0;32mwith[0m [0m_InterpolateFunctionError[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m    554[0m         [0;32mif[0m [0mcancellation_manager[0m [0;32mis[0m [0;32mNone[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0;32m--> 555[0;31m           outputs = execute.execute(
[0m[1;32m    556[0m               [0mstr[0m[0;34m([0m[0mself[0m[0;34m.[0m[0msignature[0m[0;34m.[0m[0mname[0m[0;34m)[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m
[1;32m    557[0m               [0mnum_outputs[0m[0;34m=[0m[0mself[0m[0;34m.[0m[0m_num_outputs[0m[0;34m,[0m[0;34m[0m[0;34m[0m[0m

[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py[0m in [0;36mquick_execute[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)[0m
[1;32m     57[0m   [0;32mtry[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[1;32m     58[0m     [0mctx[0m[0;34m.[0m[0mensure_initialized[0m[0;34m([0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m
[0;32m---> 59[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
[0m[1;32m     60[0m                                         inputs, attrs, num_outputs)
[1;32m     61[0m   [0;32mexcept[0m [0mcore[0m[0;34m.[0m[0m_NotOkStatusException[0m [0;32mas[0m [0me[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m

[0;31mInvalidArgumentError[0m:  Input is not invertible.
	 [[node gradient_tape/MatrixInverse (defined at /home/johannbs/Desktop/IceCube_GNN/train_likelihood.py:155) ]] [Op:__inference_train_step_2725]

Errors may have originated from an input operation.
Input Source operations connected to node gradient_tape/MatrixInverse:
 add (defined at /home/johannbs/Desktop/IceCube_GNN/loss_functions.py:101)

Function call stack:
train_step


[23;0t